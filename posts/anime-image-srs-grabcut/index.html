<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
    
    <head>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
        <title>Anime image background removal: saliency detection and matting - アレクシのブログ - Alexis's blog</title>
        
        <link rel="stylesheet" href="../../fonts/Serif/cmun-serif.css" />
        <link rel="stylesheet" href="../../fonts/Serif-Slanted/cmun-serif-slanted.css" />

        <!--BOOTSTRAP-->
        <link href="../../bootstrap/css/bootstrap.min.css" rel="stylesheet">
        <!--mobile first-->
        <meta name="viewport" content="width=device-width, initial-scale=1.0">

        <!--removed html from url but still is html-->
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />

        <!--font awesome-->
        <link href="//netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" rel="stylesheet">

        <!--fonts: allan & cardo-->
        <link href="http://fonts.googleapis.com/css?family=Droid+Serif" rel="stylesheet" type="text/css">
        <link href="http://fonts.googleapis.com/css?family=Droid+Sans" rel="stylesheet" type="text/css">

        <link href="../../css/sticky-footer-navbar.css" rel="stylesheet">

        <link href="../../css/default.css" rel="stylesheet">

        <link href="../../css/bootstrap-carousel.css" rel="stylesheet">

        <link href="../../css/inlineDisqussions.css" rel="stylesheet">

        <!--Highlight-->
        <link href="../../highlight/styles/github.css" rel="stylesheet">
        
        <!--link href="/favicon.ico" rel="shortcut icon"-->

        <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

    </head>

    <body>
        <div id="wrap">
            <nav class="navbar navbar-inverse navbar-static-top" role="navigation">
                <div class="container">
                    <!--Toggle header for mobile-->
                    <div class="navbar-header">
                        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                            <span class="sr-only">Toggle navigation</span>
                            <span class="icon-bar"></span>
                            <span class="icon-bar"></span>
                            <span class="icon-bar"></span>
                        </button>
                        <a class="navbar-brand active" href="../../" id="home">アレクシのブログ - Alexis's blog</a>
                    </div>
                    <!--normal header-->
                    <div class="navbar-collapse collapse">
                        <ul class="nav navbar-nav navbar-right">
                            <li><a href="../../archive.html"><span class="glyphicon glyphicon-pencil"></span>  Blog</a></li>
                            <li><a href="../../about.html"><span class="glyphicon glyphicon-user"></span>  About</a></li>
                            <li><a href="../../contact.html"><span class="glyphicon glyphicon-envelope"></span>  Contact</a></li>
                        </ul>
                    </div><!--/.nav-collapse -->
                </div>
            </nav>

            
            <div id="content">
                <div class="container">
                    <div class="row">
                        <div class="col-md-8">
                            <h1>Anime image background removal: saliency detection and matting</h1>
                            <div id="body">
                                <div class="info">
    <p style="font-family:CMSS; font-size:120%">Posted on May 11, 2014</p>
    <!--
        by Alexis
    -->
</div>
</br>

<p>Hi, my name is Alexis, I’m french and a Ph.D. student in computer vision at Kyushu University, Japan. I want to use this blog to document my research in a way that is (hopefully) more informal, pedagogical and interactive than a research paper :) .</p>
<p>The focus of my research is the analysis of anime images, i.e. the kind of artwork you can find in communities such as <a href="http://www.pixiv.net/">Pixiv</a> and <a href="http://www.deviantart.com/">deviantArt</a>. One of the goals I want to reach is the automated identification of characters in a color image - think facebook face tagging except for anime.</p>
<h2 id="background-removal">Background removal</h2>
<p>Unlike the face of a human being, the facial features of an anime character are not really sufficient for identification. There is simply too much variation across artists and drawing styles for that to be practical. So we’d like to get information from the entire body of the character - or whatever parts of the body happen to be in the image :p - for identification.</p>
<div class="bigcenterimgcontainer">
<div class="container-fluid">
<div class="row">
<div class="col-md-4 vcenter">
<a href="http://siguredo.deviantart.com/art/Chibi-fan-Asuka-Langley-100037700"> <img src="../../posts/anime-image-srs-grabcut/asuka_langley_1.jpg" class="img-responsive" title="Chibi fan -  Asuka Langley - by siguredo"> </a>
</div>
<div class="col-md-4 vcenter">
<a href="http://luches.deviantart.com/art/Asuka-147926773"> <img src="../../posts/anime-image-srs-grabcut/asuka_langley_6.jpg" class="img-responsive" title="Asuka - by luches"> </a>
</div>
<div class="col-md-4 vcenter">
<a href="http://qsan90.deviantart.com/art/Asuka-Shikinami-Langley-167171898"> <img src="../../posts/anime-image-srs-grabcut/asuka_langley_9.jpg" class="img-responsive" title="Asuka Shikinami Langley - by qsan90"> </a>
</div>
</div>
</div>
<div class="caption">
Examples of variations across artists and drawing styles for a single character.
</div>
</div>
<p>However, chances are the character is superimposed over some kind of background. This background rarely ever contains information relevant to character identification, so we would like to remove it as a first step. Background removal, also referred to as foreground extraction and matting, has been extensively studied in the case of natural images. There is, however, little literature about background extraction for artificial images of any kind. A well known matting algorithm called <a href="http://research.microsoft.com/pubs/67890/siggraph04-grabcut.pdf">GrabCut</a> is implemented in <a href="http://opencv.org/">OpenCV</a>, so I first tried that and it turned out to give surprisingly decent results for anime images.</p>
<div class="bigcenterimgcontainer">
<div class="container-fluid">
<div class="row">
<div class="col-md-6 vcenter">
<a href="http://dannex009.deviantart.com/art/Mikuru-Asahina-Mikuru-BEAM-62960402"> <img src="../../posts/anime-image-srs-grabcut/asahina_mikuru_0.jpg" title="Mikuru Asahina - Mikuru BEAM - by dannex09"> </a>
</div>
<div class="col-md-6 vcenter">
<img src="../../posts/anime-image-srs-grabcut/asahina_mikuru_0_grabcut.jpg">
</div>
</div>
</div>
<div class="caption">
Results of GrabCut with a manually specified bounding box.
</div>
</div>
<p>One of the issues with using GrabCut on its own is that it requires some kind of mask or bounding box for the object of interest. In the context of web artist communities, this means more user input, which is undesirable. We would like to automatically locating the objects which stand out - or are more <em>salient</em>. This is appropriately called saliency detection in the computer vision literature, and - like matting - it has mostly been applied to natural images.</p>
<p>I chose to study the saliency detection method by Hou et al. called <a href="http://www.klab.caltech.edu/~xhou/papers/cvpr07.pdf">spectral residual saliency</a>, as it is one of the most computationally efficient - only requiring a couple of 2D fast Fourier transforms, some filtering and standard linear algebra operations. Many methods perform better on <a href="http://people.csail.mit.edu/tjudd/SaliencyBenchmark/">benchmarks</a> compared to eye tracking data, however for my purposes a very rough saliency map will do just fine - GrabCut takes care of the rest.</p>
<h2 id="spectral-residual-saliency-for-anime-images">Spectral residual saliency for anime images</h2>
<p>This saliency detection method relies heavily on the Fourier transform, so here is a brief rundown of the properties we are interested in for this article. The Fourier transform, Fourier spectrum or just spectrum of an image <span class="math">\(I\)</span> is a complex valued image <span class="math">\(F(I)\)</span> of the same dimensions as <span class="math">\(I\)</span>. In a precise sense, the Fourier transform encodes information about patterns in the image according to the frequency at which they repeat - the further you go from the center of the spectrum, the higher the frequency. There is also an inverse Fourier transform <span class="math">\(F^{-1}\)</span> which reverses a Fourier transform into the original image, i.e. for any image <span class="math">\(I\)</span>, <span class="math">\(F^{-1}(F(I)) = I\)</span>. The magnitude spectrum of an image <span class="math">\(I\)</span> is the absolute value of its Fourier transform, <span class="math">\(|F(I)|\)</span>.</p>
<div class="bigcenterimgcontainer">
<div class="container-fluid">
<div class="row">
<div class="col-md-6 vcenter">
<a href="http://tinyglow.deviantart.com/art/Ginko-209756321"> <img src="ginko8grayscale.jpg" title~"Ginko - by tinyglow"> </a>
</div>
<div class="col-md-6 vcenter">
<img src="ginko8reverted.jpg">
</div>
</div>
<div class="row">
<div class="col-md-6 vcenter">
<img src="ginko8magnitude.jpg">
</div>
<div class="col-md-6 vcenter">
<img src="ginko8circled.jpg">
</div>
</div>
</div>
<div class="caption">
Images (top) and their respective magnitude spectra (bottom), log scaled and normalized for better visualization. The second image was obtained by inverse Fourier transform on the spectrum of the first, having only kept a small disk in the center. This effectively removed all high frequency patterns in the image, and produced the blurred result you can see.
</div>
</div>
<p>Basically, spectral residual saliency detects the “spikes” in the log magnitude spectrum, and says those “spikes” correspond to salient elements in the original image. To do that, it first “smooths out” the log magnitude spectrum to get its general trend, and just takes the difference between the original and the “smoothed” one. Using an inverse Fourier transform on this difference (after a bit of complex number trickery I won’t get into :p), one gets the corresponding objects in the source image - what we call a <em>saliency map</em>.</p>
<div class="bigcenterimgcontainer">
<div class="container-fluid">
<div class="row">
<div class="col-md-4 vcenter">
<a href="http://hatsunemiku93.deviantart.com/art/Hatsune-X-Miku-118398739"> <img src="miku_hatsune_0.jpg" title="Hatsune x Miku - by hatsunemiku93"> </a>
</div>
<div class="col-md-4 vcenter">
<img src="miku0logspec.jpg">
</div>
<div class="col-md-4 vcenter">
<img src="miku0smoothed.jpg">
</div>
</div>
<div class="row">
<div class="col-md-4 vcenter">
<img src="miku0residual.jpg">
</div>
<div class="col-md-4 vcenter">
<img src="mikurawsaliency.jpg">
</div>
<div class="col-md-4 vcenter">
<img src="mikufilteredsaliency.jpg">
</div>
</div>
</div>
<div class="caption">
All the steps of spectral residual saliency. From left to right, top to bottom: the source image, its magnitude spectrum on a log scale (after grayscale conversion), the same spectrum “smoothed”, the absolute difference image between the 2 previous spectra, the corresponding saliency map via inverse Fourier transform, and the same saliency map filtered.
</div>
</div>
<p>The raw saliency map seems to detect the edges of the character well. However, it fails to take into account large patches of homogeneous color on the character itself, and picks up isolated edges in the background. Blurring out the saliency map with a Gaussian filter as the original paper suggests solves both issues. <a href="http://people.csail.mit.edu/tjudd/SaliencyBenchmark/">Benchmark data</a> also shows that blurrier saliency maps perform better on natural images, and this result seems to carry to animation images.</p>
<p>After blurring, some maps still fail to account for the character in the image, picking up irrelevant text, sharp background objects and foreground frames, often on the sides of the image. Like photos however, the area of interest tends to be in the center of the picture. We therefore add a simple 2D Gaussian to the middle of the image in order to introduce a center bias. This <a href="http://people.csail.mit.edu/tjudd/SaliencyBenchmark/">has been shown</a> to improve saliency results in the case of natural images, and it seems that anime images follow this trend too.</p>
<div class="bigcenterimgcontainer">
<div class="container-fluid">
<div class="row">
<div class="col-md-6 vcenter">
<a href="http://spacecowboytv.deviantart.com/art/Mikuru-Asahina-63493581"> <img src="asahina9.jpg" title="Mikuru Asahina by spacecowboytv"> </a>
</div>
<div class="col-md-6 vcenter">
<a href="http://spacecowboytv.deviantart.com/art/Mikuru-Asahina-63493581"> <img src="asahina9centered.jpg" title="Mikuru Asahina by spacecowboytv"> </a>
</div>
</div>
</div>
<div class="caption">
Without center prior, and with center prior on the right. Although poor Mikuru lost an eye in the process, it’s quite a bit better for our purposes.
</div>
</div>
<p>The implementation of GrabCut in OpenCV then expects a mask indicating what is “probably background” and “probably foreground”. To do this, we simply threshold the saliency map - if the saliency of a pixel is below a certain value, then it’s probably background. Otherwise, it’s probably foreground. From there, GrabCut does a remarkable job without further user inputs :) .</p>
<h2 id="dealing-with-color-using-principal-component-analysis">Dealing with color using principal component analysis</h2>
<p>The previous algorithm, however, only works on grayscale images, as there is no such thing as a Fourier transform of a color image. Which is a shame, as most anime images are colored. The creators of the spectral residual saliency method suggest applying the algorithm to each RGB color channel individually, then combine the resulting saliency maps into one somehow. I’m however not a big fan of RGB, and I’ll be working using CIE <span class="math">\(L^*a^*b^*\)</span> color, which usually has much better properties for computer vision tasks. Indeed, RGB is essentially intended for digital display, and as a color space doesn’t have such great properties - namely, if you take 2 RGB values <span class="math">\((r_1,g_1,b_1)\)</span> and <span class="math">\((r_2,g_2,b_2)\)</span> and compute the Euclidan distance between them, this doesn’t really tell you anything about how a human would perceive the 2 colors comparatively.</p>
<p>However, <span class="math">\(L^*a^*b^*\)</span> was designed specifically so the Euclidean distance between colors tells you something - if it close to 0, then the colors appear almost the same to the human eye. If it is big, then it means a human would be able to distinguish them easily. Since anime images were designed to be viewed by non-colorblind humans this can be a meaningful metric.</p>
<p>Anyway, running saliency detection on each color channel is still not guaranteed to be very meaningful even in <span class="math">\(L^*a^*b^*\)</span> color space. So my first idea was to find the best possible conversion to grayscale for a given image, the one that would keep the most information about the original image. Converting a color image to grayscale is a special case of what we fancy computer scientists call dimensionality reduction, as we go from 3D (RGB or <span class="math">\(L^*a^*b^*\)</span>) to 1D. A very well-known dimensionality reduction method is called principal component analysis (PCA), and so I tried that out.</p>
<p>Essentially, PCA takes a bunch of points in space - 3D space in our case - and finds the orientation of this bunch of points. This orientation can be defined as the direction at which the points show the most variation from the average. So if we project all the points on the line defined by this direction, we get the 1D data (a point on a line) which preserves the most variation among these points. Translating this to image vocabulary, if we apply PCA to <span class="math">\(L^*a^*b^*\)</span> pixel values in our image, we get new grayscale pixel values which preserve the most variation in human perception - as that is what variation in <span class="math">\(L^*a^*b^*\)</span> color space means in some sense.</p>
<div class="bigcenterimgcontainer">
<iframe src="../../posts/anime-image-srs-grabcut/PCAScatterPlot.html" class="webgl">
</iframe>
<div class="container-fluid">
<div class="row">
<div class="col-md-12">
<img src="fallbackScatterPlot.jpg" class="nowebgl img-responsive">
</div>
</div>
</div>
<div class="caption">
3D scatter plot of <span class="math">\(L^*a^*b^*\)</span> pixel values in an anime image. Gray axis is <span class="math">\(L^*\)</span>, pink is <span class="math">\(a^*\)</span> and green is <span class="math">\(b^*\)</span>. In blue, we have the 3 vectors <span class="math">\(V_1\)</span>, <span class="math">\(V_2\)</span> and <span class="math">\(V_3\)</span>, scaled by <span class="math">\(x_1\)</span>, <span class="math">\(x_2\)</span> and <span class="math">\(x_3\)</span> respectively. Click to rotate the plot around.
</div>
</div>

<p>So, we have an optimal grayscale conversion, cool ! We’re still wasting information - i.e. all the variation that’s not along this line we found. It turns out that PCA can allow us to - optimally - project 3D data in 2D space, and even 3D space, which allows us to capture more information about the original. Let me explain.</p>
<p>Let’s identify the first direction we found by a vector, which we’ll call <span class="math">\(V_1\)</span>. This vector already accounts for all the data that’s parallel to it, so what we’re really interested in what’s left. It follows that what’s left is whatever is orthogonal to <span class="math">\(V_1\)</span>. In 3D space, that’s a plane called the orthogonal plane, and so we can project our data on this plane to obtain 2D data. As I mentioned earlier, PCA not only works on 3D data but also 2D data, so we can apply the same process once again to find the orientation of the data in this 2D plane, which we’ll represent with a vector <span class="math">\(V_2\)</span>. And we can repeat the process one more time, taking the space that’s orthogonal to both <span class="math">\(V_1\)</span> and <span class="math">\(V_2\)</span>, which is just a 1D line - which follows our last vector <span class="math">\(V_3\)</span>. Since we’re in 3D space, and those 3 vectors are orthogonal to each other, all the information in the image is kept by projecting data along these 3 vectors - which give us 3 channels, i.e. a new color image.</p>
<p>So we now have a color space in which each channel capture the most variation in the original image, so it kind of makes sense to apply spectral residual saliency to each channel separately. This, however, still doesn’t tell us how we should combine the 3 resulting saliency maps. It turns out that we accidentally found that information along the way: PCA also tells exactly how much each of the vectors <span class="math">\(V_1\)</span>, <span class="math">\(V_2\)</span> and <span class="math">\(V_3\)</span> contribute to the variations in pixel value with real numbers <span class="math">\(x_1\)</span>, <span class="math">\(x_2\)</span> and <span class="math">\(x_3\)</span>. So it makes sense to combine the corresponding saliency maps - which we’ll call <span class="math">\(S_1\)</span>, <span class="math">\(S_2\)</span> and <span class="math">\(S_3\)</span> - by weighting them by these values into an average final saliency map <span class="math">\(S\)</span>:</p>
<p><span class="math">\[S = \frac{x_1S_1 + x_2S_2 + x_3S_3}{x_1 + x_2 + x_3}\]</span></p>
<h2 id="whats-next">What’s next?</h2>
<p>In this article, I’ve outlined the results of my research so far on background removal for anime images. I will soon run benchmarks comparing how these different methods perform, and how much each additional “trick” brings to background removal quality. Further details into the method developed, as well as benchmark results and analysis, will be included in an upcoming paper.</p>
<p>In further work, I will study and apply object detection methods to improve the current method. I hope to detect the face, arms, legs and other features of the character in order to better locate it in the image, and subsequently get better background removals.</p>
<h2 id="acknowledgments-and-thanks">Acknowledgments and thanks</h2>
<p>This blog is based off the kickass <a href="http://oinkina.github.io/">Lambda Oinks</a> by Oinkina who very kindly allowed me to fork the source code of her blog :) . I’d also like to thank my advisor Sakamoto sensei, many of the ideas in this article originated from his fertile mind. Huge thanks to Joseph Kochmann, Hugo Duprat and Matthias Paillard for their help with writing and helping make this blog post more accessible.</p>
<p>All the anime images in this post were found on <a href="http://www.deviantart.com/">deviantArt</a>, and I do not own the rights to any of them or to the characters depicted. It is my hope that artists and copyright holder will understand that no harm is intended as their usage here is purely pedagogical. Please don’t sue me :p . Seriously, if you want me to remove any of the images here, please <a href="http://localhost:8000/contact.html">contact me</a>, and it will be done shortly.</p>

<div id="disqus_thread"></div>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
<script src="http://code.jquery.com/jquery-1.10.1.min.js"></script>
<script src="../../js/disqus.js"></script>

                            </div>
                        </div>
                        <div class="col-md-4" id="toc-wrapper">
                        </div>
                    </div>
                </div>
            </div>
        

            <div id="footer">
                <div class="container">
                    Based on <a href="http://oinkina.github.io/">Lambda Oinks</a> by <a href="https://github.com/oinkina">Oinkina</a>, using 
                    <a href="http://jaspervdj.be/hakyll">Hakyll</a>,
                    <a href="http://getbootstrap.com/">Bootstrap</a>, 
                    <a href="http://www.mathjax.org/">MathJax</a>, and
                    <a href="http://disqus.com/">Disqus</a>.
                </div>
            </div>
        </div>

    <!-- jQuery-->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
    <script src="http://code.jquery.com/jquery-1.10.1.min.js"></script>

    <script src="../../bootstrap/js/bootstrap.min.js"></script>

    <script src="../../highlight/highlight.pack.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>

    <script src="../../js/bootstrap-carousel.js"></script>
    <script src="../../js/toc.js"></script>
    <script src="../../js/webGLFallback.js"></script>

    <noscript>Enable JavaScript for footnotes, Disqus comments, and other cool stuff.</noscript>

    </body>

</html>
