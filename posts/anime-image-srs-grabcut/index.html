<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
    
    <head>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
        <title>Anime image background removal: saliency detection and matting (Draft) - アレクシのブログ - Alexis's blog</title>
        
        <link rel="stylesheet" href="../../fonts/Serif/cmun-serif.css" />
        <link rel="stylesheet" href="../../fonts/Serif-Slanted/cmun-serif-slanted.css" />

        <!--BOOTSTRAP-->
        <link href="../../bootstrap/css/bootstrap.min.css" rel="stylesheet">
        <!--mobile first-->
        <meta name="viewport" content="width=device-width, initial-scale=1.0">

        <!--removed html from url but still is html-->
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />

        <!--font awesome-->
        <link href="//netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" rel="stylesheet">

        <!--fonts: allan & cardo-->
        <link href="http://fonts.googleapis.com/css?family=Droid+Serif" rel="stylesheet" type="text/css">
        <link href="http://fonts.googleapis.com/css?family=Droid+Sans" rel="stylesheet" type="text/css">

        <link href="../../css/sticky-footer-navbar.css" rel="stylesheet">

        <link href="../../css/default.css" rel="stylesheet">

        <link href="../../css/bootstrap-carousel.css" rel="stylesheet">

        <link href="../../css/inlineDisqussions.css" rel="stylesheet">

        <!--Highlight-->
        <link href="../../highlight/styles/github.css" rel="stylesheet">
        
        <!--link href="/favicon.ico" rel="shortcut icon"-->

        <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

    </head>

    <body>
        <div id="wrap">
            <nav class="navbar navbar-inverse navbar-static-top" role="navigation">
                <div class="container">
                    <!--Toggle header for mobile-->
                    <div class="navbar-header">
                        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                            <span class="sr-only">Toggle navigation</span>
                            <span class="icon-bar"></span>
                            <span class="icon-bar"></span>
                            <span class="icon-bar"></span>
                        </button>
                        <a class="navbar-brand active" href="../../" id="home">アレクシのブログ - Alexis's blog</a>
                    </div>
                    <!--normal header-->
                    <div class="navbar-collapse collapse">
                        <ul class="nav navbar-nav navbar-right">
                            <li><a href="../../archive.html"><span class="glyphicon glyphicon-pencil"></span>  Blog</a></li>
                            <li><a href="../../about.html"><span class="glyphicon glyphicon-user"></span>  About</a></li>
                            <li><a href="../../contact.html"><span class="glyphicon glyphicon-envelope"></span>  Contact</a></li>
                        </ul>
                    </div><!--/.nav-collapse -->
                </div>
            </nav>

            
            <div id="content">
                <div class="container">
                    <div class="row">
                        <div class="col-md-8">
                            <h1>Anime image background removal: saliency detection and matting (Draft)</h1>
                            <div id="body">
                                <div class="info">
    <p style="font-family:CMSS; font-size:120%">Posted on April 19, 2014</p>
    <!--
        by Alexis
    -->
</div>
</br>

<p>Hi, my name is Alexis, I’m french and a Ph.D. student in computer vision at Kyushu University, Japan. I want to use this blog to document my research in a way that is (hopefully) more informal, pedagogical and interactive than a research paper :) .</p>
<p>The focus of my research is the analysis of anime images, i.e. the kind of artwork you can find in communities such as <a href="http://www.pixiv.net/">Pixiv</a> and <a href="http://www.deviantart.com/">deviantArt</a>. One of the goals I want to reach is the automated identification of characters in a color image - think facebook face tagging except for anime.</p>
<h2 id="background-removal">Background removal</h2>
<p>Unlike the face of a human being, the facial features of an anime character are not really sufficient for identification. There is simply too much variation across artists and drawing styles for that to be practical. So we’d like to get information from the entire body of the character - or whatever parts of the body happen to be in the image :p - for identification.</p>
<div class="bigcenterimgcontainer">
<a href="http://siguredo.deviantart.com/art/Chibi-fan-Asuka-Langley-100037700"> <img src="../../posts/anime-image-srs-grabcut/asuka_langley_1.jpg" style="width: 32%;height: auto; display: inline;" title="Chibi fan -  Asuka Langley - by siguredo"> </a> <a href="http://luches.deviantart.com/art/Asuka-147926773"> <img src="../../posts/anime-image-srs-grabcut/asuka_langley_6.jpg" style="width: 32%;height: auto; display: inline;" title="Asuka - by luches"> </a> <a href="http://qsan90.deviantart.com/art/Asuka-Shikinami-Langley-167171898"> <img src="../../posts/anime-image-srs-grabcut/asuka_langley_9.jpg" style="width: 32%;height: auto; display: inline;" title="Asuka Shikinami Langley - by qsan90"> </a>
<div class="caption">
Examples of variations across artists and drawing styles for a single character.
</div>
</div>
<p>However, chances are the character is superimposed over some kind of background. This background rarely ever contains information relevant to character identification, so we would like to remove it as a first step. Background removal, also referred to as foreground extraction and matting, has been extensively studied in the case of natural images. There is, however, little literature about background extraction for artificial images of any kind. A well known matting algorithm called <a href="http://research.microsoft.com/pubs/67890/siggraph04-grabcut.pdf">GrabCut</a> is implemented in <a href="http://opencv.org/">OpenCV</a>, so I first tried that and it turned out to give surprisingly decent results for anime images.</p>
<div class="bigcenterimgcontainer">
<a href="http://dannex009.deviantart.com/art/Mikuru-Asahina-Mikuru-BEAM-62960402"> <img src="../../posts/anime-image-srs-grabcut/asahina_mikuru_0.png" style="width: 49%;height: auto; display: inline;" title="Mikuru Asahina - Mikuru BEAM - by dannex09"> </a> <img src="../../posts/anime-image-srs-grabcut/asahina_mikuru_0_grabcut.png" style="width: 49%;height: auto; display: inline;">
<div class="caption">
Results of GrabCut with a manually specified bounding box.
</div>
</div>
<p>One of the issues with using GrabCut on its own is that it requires some kind of mask or bounding box for the object of interest. In the context of web artist communities, this means more user input, which is undesirable. We would like to automatically locating the objects which stand out - or are more <em>salient</em>. This is appropriately called saliency detection in the computer vision literature, and - like matting - it has mostly been applied to natural images.</p>
<p>I chose to study the saliency detection method by Hou et al. called <a href="http://www.klab.caltech.edu/~xhou/papers/cvpr07.pdf">spectral residual saliency</a>, as it is one of the most computationally efficient - only requiring a couple of 2D fast Fourier transforms, some filtering and standard linear algebra operations. Many methods perform better on <a href="http://people.csail.mit.edu/tjudd/SaliencyBenchmark/">benchmarks</a> compared to eye tracking data, however for my purposes a very rough saliency map will do just fine - GrabCut takes care of the rest.</p>
<h2 id="spectral-residual-saliency-for-anime-images">Spectral residual saliency for anime images</h2>
<p>This saliency detection method relies heavily on the Fourier transform, so here is a brief rundown of the properties we are interested in for this article. The Fourier transform, Fourier spectrum or just spectrum of an image <span class="math">\(I\)</span> is a complex valued image <span class="math">\(F(I)\)</span> of the same dimensions as <span class="math">\(I\)</span>. In a precise sense, the Fourier transform encodes information about patterns in the image according to the frequency at which they repeat. There is also an inverse Fourier transform <span class="math">\(F^{-1}\)</span> which reverses a Fourier transform into the original image, i.e. for any image <span class="math">\(I\)</span>, <span class="math">\(F^{-1}(F(I)) = I\)</span>. The magnitude spectrum of an image <span class="math">\(I\)</span> is the absolute value of its Fourier transform, <span class="math">\(|F(I)|\)</span>.</p>
<div class="bigcenterimgcontainer">
<a href="http://tinyglow.deviantart.com/art/Ginko-209756321"> <img src="ginko8grayscale.png" style="width: 49%;height: auto; display: inline;" title~"Ginko - by tinyglow"> <img src="ginko8reverted.png" style="width: 49%;height: auto; display: inline;"> </a>
</div>
<div class="bigcenterimgcontainer">
<img src="ginko8magnitude.png" style="width: 49%;height: auto; display: inline;"> <img src="ginko8circled.png" style="width: 49%;height: auto; display: inline;">
<div class="caption">
Images (top row) and their respective magnitude spectra (bottom row), log scaled and normalized for better visualization. The second image was obtained by inverse Fourier transform on the spectrum of the first, having only kept a small disk in the center. This effectively removed all high frequency patterns in the image, and produced the blurred result you can see.
</div>
</div>
<p>Basically, spectral residual saliency detects the “spikes” in the magnitude spectrum, and says those “spikes” correspond to salient elements in the original image. To do that, it first “smooths out” the magnitude spectrum to get its general trend, and just takes the difference between the original magnitude spectrum and the “smoothed” one. Using an inverse Fourier transform on this difference (with a bit of additional trickery I won’t get into :p), one gets the corresponding objects in the source image - what we call a <em>saliency map</em>.</p>
<p>The raw saliency map seems to detect the edges of the character well. However, it fails to take into account large patches of homogeneous color on the character itself, and picks up isolated edges in the background. Blurring out the saliency map with a Gaussian filter as the original paper suggests solves both issues. <a href="http://people.csail.mit.edu/tjudd/SaliencyBenchmark/">Benchmark data</a> also show that blurrier saliency maps perform better on natural images, and this result seems to carry to animation images.</p>
<div class="bigcenterimgcontainer">
<a href="http://hatsunemiku93.deviantart.com/art/Hatsune-X-Miku-118398739"> <img src="miku_hatsune_0.jpg" style="width: 32%;height: auto; display: inline;" title="Hatsune x Miku - by hatsunemiku93"> </a> <img src="mikurawsaliency.png" style="width: 32%;height: auto; display: inline;"> <img src="mikufilteredsaliency.png" style="width: 32%;height: auto; display: inline;">
<div class="caption">
Source image, raw saliency map and filtered saliency map.
</div>
</div>
<p>After blurring, some maps still fail to account for the character in the image, picking up irrelevant text, sharp background objects and foreground frames, often on the sides of the image. Like photos however, the area of interest tends to be in the center of the picture. We therefore add a simple 2D Gaussian to the middle of the image in order to introduce a center bias. This <a href="http://people.csail.mit.edu/tjudd/SaliencyBenchmark/">has been shown</a> to improve saliency results in the case of natural images, and it seems that anime images follow this trend too.</p>
<div class="bigcenterimgcontainer">
<a href="http://spacecowboytv.deviantart.com/art/Mikuru-Asahina-63493581"> <img src="asahina9.png" style="width: 45%;height: auto; display: inline;" title="Mikuru Asahina by spacecowboytv"> </a> <a href="http://spacecowboytv.deviantart.com/art/Mikuru-Asahina-63493581"> <img src="asahina9centered.png" style="width: 45%;height: auto; display: inline;" title="Mikuru Asahina by spacecowboytv"> </a>
<div class="caption">
Without center prior on the left, with center prior on the right. Although poor Mikuru lost an eye in the process, it’s quite a bit better for our purposes.
</div>
</div>
<p>The implementation of GrabCut in OpenCV then expects a mask indicating what is “probably background” and “probably foreground”. To do this, we simply threshold the saliency map - if the saliency of a pixel is below a certain value, then it’s probably background. Otherwise, it’s probably foreground. From there, GrabCut does a remarkable job without further user inputs :) .</p>
<h2 id="dealing-with-color-using-principal-component-analysis">Dealing with color using principal component analysis</h2>
<p>So far, I’ve only dealt with grayscale images. In the case of color images, the original paper suggests applying the algorithm to each color channel separately - typically RGB. It seemed to me that the RGB color space is quite ill-suited to the task, being intended for computer display. Switching to a color space like <a href="http://en.wikipedia.org/wiki/Lab_color_space"><span class="math">\(L^*a^*b^*\)</span></a>, where euclidean distance between pixel values is related to human perception, is much more interesting. Still, it is not clear that applying saliency detection to each layer individually is meaningful in any way.</p>
<p>At first, I used the default grayscale conversion from OpenCV, and just applied the algorithm to this one converted image. However, this is only designed to look good to the human eye, and is certainly not optimized for one individual image. Thinking about how one could find the best grayscale conversion for a single image, I thought about using <a href="http://en.wikipedia.org/wiki/Principal_component_analysis">principal component analysis</a> (PCA) for dimensionality reduction from 3 to 1 channels. Let’s say we have a 3-channel image with <span class="math">\(n\)</span> pixels, then we can construct the data matrix <span class="math">\(X \in \mathbb{R}^{n \times 3}\)</span> where we put each pixel value of the image in its own row - minus the average pixel value in the image. Applying PCA results in computing the eigenvector <span class="math">\(\phi_1\)</span> corresponding to the largest eigenvalue <span class="math">\(\lambda_1\)</span> of the covariance matrix <span class="math">\(X^TX \in \mathbb{R}^{3 \times 3}\)</span>. Projecting each pixel into the 1D space spanned by <span class="math">\(\phi_1\)</span>, we get the grayscale image which encodes the most information about pixel value variance in the original image.</p>
<div class="bigcenterimgcontainer">
<iframe class="mathbox" src="../../posts/anime-image-srs-grabcut/PCAScatterPlot.html">
</iframe>
<div class="caption">
3D scatter plot of <span class="math">\(L^*a^*b^*\)</span> pixel values in an anime image. Gray axis is <span class="math">\(L^*\)</span>, pink is <span class="math">\(a^*\)</span> and green is <span class="math">\(b^*\)</span>. In blue, the principal components scaled by the square root of their corresponding eigenvalues.
</div>
</div>

<p>But we are still losing a lot of information by converting into a grayscale image. This is easily solved by generalizing the above, and taking the 3 eigenvectors <span class="math">\(\phi_1\)</span>, <span class="math">\(\phi_2\)</span> and <span class="math">\(\phi_3\)</span> of <span class="math">\(X^TX\)</span> corresponding to eigenvalues <span class="math">\(\lambda_1 \geq \lambda_2 \geq \lambda_3\)</span>, and projecting data into the 3D space spanned by said eigenvectors, resulting in a new 3-channels image. Said 3D space is in fact the same space as the original, simply with a different choice of basis - hence no loss of data. However we now crucially have information about how much each channel contributes to the variance in the image - namely, eigenvalues <span class="math">\(\lambda_1 \geq \lambda_2 \geq \lambda_3\)</span>. So if we compute saliency maps <span class="math">\(S_1\)</span>, <span class="math">\(S_2\)</span> and <span class="math">\(S_3\)</span> for each channel of this 3D image, it makes sense to combine them in a final saliency map <span class="math">\(S\)</span> using the following weighted mean:</p>
<p><span class="math">\[
S = \frac{\sqrt{\lambda_1}S_1 + \sqrt{\lambda_2}S_2 + \sqrt{\lambda_3}S_3}{\sqrt{\lambda_1} + \sqrt{\lambda_2} + \sqrt{\lambda_3}}
\]</span></p>
<p>Writing this all up, I realized that the PCA step is somewhat unnecessary, as one can simply measure variance along all 3 axis of <span class="math">\(L^*a^*b^*\)</span> color space and use this instead of the eigenvalues to construct the final saliency map.</p>
<p>However, I noted that <span class="math">\(\lambda_3\)</span> is often orders of magnitude smaller than the other eigenvalues, so it might be very beneficial from a computational standpoint to drop the 3rd channel entirely. Indeed, using PCA here is not very computationally intensive, as it’s just 2 <span class="math">\(O(n)\)</span> matrix products and finding the eigenvectors and eigenvalues on a 3 by 3 matrix in <span class="math">\(O(1)\)</span>. In fact, clever libraries like numpy allow you to avoid any kind of data copying for pixel values. On the other hand, applying spectral residual saliency on one more channel requires running 2 <a href="http://en.wikipedia.org/wiki/Fast_fourier_transform">fast Fourier transforms</a> on <span class="math">\(n\)</span>-pixels images, which is an <span class="math">\(O(n\log(n))\)</span> operation.</p>
<p>As far as I can tell, this technique can be applied to many other kinds of computer vision techniques which operate on single image layers before combining them. This includes other methods based on the Fourier transform, edge detection comes to mind. I feel like someone probably came up with this before I did, if anyone more knowledgeable could point me in the right direction that would be awesome, as I would like to give proper credit :p .</p>
<h2 id="whats-next">What’s next?</h2>
<p>In this article, I’ve outlined the results of my research so far on background removal for anime images. I will soon run benchmarks comparing how these different methods perform, and how much each additional “trick” brings to background removal quality. Further details into the method developed, as well as benchmark results and analysis, will be included in an upcoming paper.</p>
<h2 id="acknowledgments-and-thanks">Acknowledgments and thanks</h2>
<p>This blog is based off the kickass <a href="http://oinkina.github.io/">Lambda Oinks</a> by Oinkina who very kindly allowed me to fork the source code of her blog :) . I’d also like to thank my advisor Sakamoto sensei, many of the ideas in this article originated from his fertile mind.</p>
<p>All the anime images in this post were found on <a href="http://www.deviantart.com/">deviantArt</a>, and I do not own the rights to any of them or to the characters depicted. It is my hope that artists and copyright holder will understand that no harm is intended as their usage here is purely pedagogical. Please don’t sue me :p . Seriously, if you want me to remove any of the images here, please <a href="http://localhost:8000/contact.html">contact me</a>, and it will be done shortly.</p>

<div id="disqus_thread"></div>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
<script src="http://code.jquery.com/jquery-1.10.1.min.js"></script>

<script src="../../js/inlineDisqussions.js"></script>
<script src="../../js/disqus.js"></script>

                            </div>
                        </div>
                        <div class="col-md-4" id="toc-wrapper">
                        </div>
                    </div>
                </div>
            </div>
        

            <div id="footer">
                <div class="container">
                    Based on <a href="http://oinkina.github.io/">Lambda Oinks</a> by <a href="https://github.com/oinkina">Oinkina</a>, using 
                    <a href="http://jaspervdj.be/hakyll">Hakyll</a>,
                    <a href="http://getbootstrap.com/">Bootstrap</a>, 
                    <a href="http://www.mathjax.org/">MathJax</a>, and
                    <a href="http://disqus.com/">Disqus</a>.
                </div>
            </div>
        </div>

    <!-- jQuery-->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
    <script src="http://code.jquery.com/jquery-1.10.1.min.js"></script>

    <script src="../../bootstrap/js/bootstrap.min.js"></script>

    <script src="../../highlight/highlight.pack.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>

    <script src="../../js/footnotes.js"></script>
    <script src="../../js/bootstrap-carousel.js"></script>
    <script src="../../js/inlineDisqussions.js"></script>
    <script src="../../js/toc.js"></script>

    <script src="../../js/analytics.js"></script>

    <noscript>Enable JavaScript for footnotes, Disqus comments, and other cool stuff.</noscript>

    </body>

</html>
