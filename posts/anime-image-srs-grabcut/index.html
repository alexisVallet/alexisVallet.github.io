<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
    
    <head>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
        <title>Anime image background removal: saliency detection and matting (Draft) - アレクシのブログ - Alexis's blog</title>
        
        <link rel="stylesheet" href="../../fonts/Serif/cmun-serif.css" />
        <link rel="stylesheet" href="../../fonts/Serif-Slanted/cmun-serif-slanted.css" />

        <!--BOOTSTRAP-->
        <link href="../../bootstrap/css/bootstrap.min.css" rel="stylesheet">
        <!--mobile first-->
        <meta name="viewport" content="width=device-width, initial-scale=1.0">

        <!--removed html from url but still is html-->
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />

        <!--font awesome-->
        <link href="//netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" rel="stylesheet">

        <!--fonts: allan & cardo-->
        <link href="http://fonts.googleapis.com/css?family=Droid+Serif" rel="stylesheet" type="text/css">
        <link href="http://fonts.googleapis.com/css?family=Droid+Sans" rel="stylesheet" type="text/css">

        <link href="../../css/sticky-footer-navbar.css" rel="stylesheet">

        <link href="../../css/default.css" rel="stylesheet">

        <link href="../../css/bootstrap-carousel.css" rel="stylesheet">

        <link href="../../css/inlineDisqussions.css" rel="stylesheet">

        <!--Highlight-->
        <link href="../../highlight/styles/github.css" rel="stylesheet">
        
        <!--link href="/favicon.ico" rel="shortcut icon"-->

        <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

    </head>

    <body>
        <div id="wrap">
            <nav class="navbar navbar-inverse navbar-static-top" role="navigation">
                <div class="container">
                    <!--Toggle header for mobile-->
                    <div class="navbar-header">
                        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                            <span class="sr-only">Toggle navigation</span>
                            <span class="icon-bar"></span>
                            <span class="icon-bar"></span>
                            <span class="icon-bar"></span>
                        </button>
                        <a class="navbar-brand active" href="../../" id="home">アレクシのブログ - Alexis's blog</a>
                    </div>
                    <!--normal header-->
                    <div class="navbar-collapse collapse">
                        <ul class="nav navbar-nav navbar-right">
                            <li><a href="../../archive.html"><span class="glyphicon glyphicon-pencil"></span>  Blog</a></li>
                            <li><a href="../../about.html"><span class="glyphicon glyphicon-user"></span>  About</a></li>
                            <li><a href="../../contact.html"><span class="glyphicon glyphicon-envelope"></span>  Contact</a></li>
                        </ul>
                    </div><!--/.nav-collapse -->
                </div>
            </nav>

            
            <div id="content">
                <div class="container">
                    <div class="row">
                        <div class="col-md-8">
                            <h1>Anime image background removal: saliency detection and matting (Draft)</h1>
                            <div id="body">
                                <div class="info">
    <p style="font-family:CMSS; font-size:120%">Posted on April 19, 2014</p>
    <!--
        by Alexis
    -->
</div>
</br>

<p>Hi, my name is Alexis, I’m french and a Ph.D. student in computer vision at Kyushu University, Japan. I want to use this blog to document my research in a way that is (hopefully) more informal, pedagogical and interactive than a research paper :) .</p>
<p>The focus of my research is the analysis of anime images, i.e. the kind of artwork you can find in communities such as <a href="http://www.pixiv.net/">Pixiv</a> and <a href="http://www.deviantart.com/">deviantArt</a>. One of the goals I want to reach is the automated identification of characters in a color image - think facebook face tagging except for anime.</p>
<h2 id="background-removal">Background removal</h2>
<p>Unlike the face of a human being, the facial features of an anime character are not really sufficient for identification. There is simply too much variation across artists and drawing styles for that to be practical. So we’d like to get information from the entire body of the character - or whatever parts of the body happen to be in the image :p - for identification.</p>
<div class="bigcenterimgcontainer">
<a href="http://siguredo.deviantart.com/art/Chibi-fan-Asuka-Langley-100037700"> <img src="../../posts/anime-image-srs-grabcut/asuka_langley_1.jpg" style="width: 30%;height: auto; display: inline;" title="Chibi fan -  Asuka Langley - by siguredo"> </a> <a href="http://luches.deviantart.com/art/Asuka-147926773"> <img src="../../posts/anime-image-srs-grabcut/asuka_langley_6.jpg" style="width: 30%;height: auto; display: inline;" title="Asuka - by luches"> </a> <a href="http://qsan90.deviantart.com/art/Asuka-Shikinami-Langley-167171898"> <img src="../../posts/anime-image-srs-grabcut/asuka_langley_9.jpg" style="width: 30%;height: auto; display: inline;" title="Asuka Shikinami Langley - by qsan90"> </a>
<div class="caption">
Examples of variations across artists and drawing styles for a single character.
</div>
</div>
<p>However, chances are the character is superimposed over some kind of background. This background rarely ever contains information relevant to character identification, so we would like to remove it as a first step. Background removal, also referred to as foreground extraction and matting, has been extensively studied in the case of natural images. There is, however, little literature about background extraction for artificial images of any kind. A well known matting algorithm called <a href="http://research.microsoft.com/pubs/67890/siggraph04-grabcut.pdf">GrabCut</a> is implemented in <a href="http://opencv.org/">OpenCV</a>, so I first tried that and it turned out to give surprisingly decent results for anime images.</p>
<div class="bigcenterimgcontainer">
<a href="http://dannex009.deviantart.com/art/Mikuru-Asahina-Mikuru-BEAM-62960402"> <img src="../../posts/anime-image-srs-grabcut/asahina_mikuru_0.png" style="width: 45%;height: auto; display: inline;" title="Mikuru Asahina - Mikuru BEAM - by dannex09"> </a> <img src="../../posts/anime-image-srs-grabcut/asahina_mikuru_0_grabcut.png" style="width: 45%;height: auto; display: inline;">
<div class="caption">
Results of GrabCut with a manually specified bounding box.
</div>
</div>
<p>One of the issues with using GrabCut on its own is that it requires some kind of mask or bounding box for the object of interest. In the context of web artist communities, this means more user input - which is undesirable. Automatically identifying the objects of interest in an image is often called saliency detection in the computer vision literature, and - like matting - it has mostly been applied to natural images.</p>
<h2 id="spectral-residual-saliency-for-anime-images">Spectral residual saliency for anime images</h2>
<p>I chose to study the saliency detection method by Hou et al. called <a href="http://www.klab.caltech.edu/~xhou/papers/cvpr07.pdf">spectral residual saliency</a>, as it is one of the most computationally efficient - only requiring a couple of 2D discrete Fourier transforms, some filtering and standard linear algebra operations. Many methods perform better on <a href="http://people.csail.mit.edu/tjudd/SaliencyBenchmark/">benchmarks</a> compared to eye tracking data, however for my purposes a very rough saliency map will do just fine - GrabCut takes care of the rest.</p>
<p>Its theoretical justification, however, relies on a result about the statistics of natural images - namely, the scale invariance result by <a href="https://cs.uwaterloo.ca/~mannr/cs886-w10/Ruderman-statistics.pdf">Ruderman</a>. In a nutshell, the result implies that plotting the average magnitude Fourier spectrum of all natural image against frequency <span class="math">\(f\)</span> something that follows a <span class="math">\(1/f\)</span> curve. To check whether this result also holds for anime images - or a result strong enough for our purposes - I went ahead and ran some statistics of my own. I hacked together a <a href="https://github.com/alexisVallet/aci-python/blob/6eac9fec04132e71184620946d1228342c3816a6/deviantArtScraper.py">scraper</a> for deviantArt’s RSS feed to painlessly constitute a data set of 200 color anime images with only one character in each image.</p>
<p>It is not obvious how one would compute the magnitude spectrum of an image against frequency. Indeed, the 2D Fourier transform of a continuous image <span class="math">\(I\)</span> is an another image <span class="math">\(F(I)\)</span> with the exact same dimensions. However, we want a simple line plot with <em>frequency</em> on the <span class="math">\(x\)</span> axis and intensity on the <span class="math">\(y\)</span> axis. To do that, let us remember that on the 2D Fourier transform frequencies are distributed over circles centered around the 0 frequency point. So to get only one intensity value for a given frequency, we take the average over all the values of this circle.</p>
<div class="bigcenterimgcontainer">
<img src="fourier.png">
<div class="caption">
Magnitude spectrum of an image - log scaled and shifted for convenience. The center point is the 0 frequency, which corresponds to the average pixel intensity in the source image. From there each circle of radius <span class="math">\(f\)</span> corresponds to the patterns of frequency <span class="math">\(f\)</span>.
</div>
</div>
<p>Running this process for all 200 images in the data set, and averaging out the magnitude spectrum gives us something that looks a suspiciously like a <span class="math">\(\frac{1}{f}\)</span> curve. Of course, individual images - although they seem to follow the same trend - certainly don’t have such a smooth spectrum.</p>
<div class="bigcenterimgcontainer">
<img src="average-magnitude-spectrum-zeroless.png" style="width: 49%;height: auto; display: inline;"> <img src="log-average-magnitude-spectrum-zeroless.png" style="width: 49%;height: auto; display: inline;">
<div class="caption">
Average magnitude spectrum of 200 anime images, untouched (left) and log-scaled (right). Note that we subtracted the average brightness to each image before computations to better show the trend. Indeed, the 0 frequency corresponding to the average brightness, has a very large value which is consistent with a <span class="math">\(\frac{1}{f}\)</span> curve, so everything else appeared squished out.
</div>
</div>
<p>Indeed, the point of the algorithm is that on a log scale, the irregularities in the spectrum correspond to salient objects in the image. Since the log spectrum follows an approximately linear pattern, simply taking anything sticking out after applying an average filter gives us a satisfying saliency map through an inverse DFT.</p>
<p>The raw saliency map seems to detect the edges of the character well. However, it fails to take into account large patches of homogeneous color on the character itself, and picks up isolated edges in the background. Blurring out the saliency map with a Gaussian filter as the original paper suggests solves both issues. <a href="http://people.csail.mit.edu/tjudd/SaliencyBenchmark/">Benchmark data</a> also show that blurry saliency maps perform better on natural images, and this result seems to carry to animation images.</p>
<p>After blurring, some maps still fail to account for the character in the image, picking up irrelevant text, sharp background objects and foreground frames. Like photos however, the area of interest tends to be in the center of the picture. We therefore add a simple 2D Gaussian to the middle of the image in order to introduce a center bias. This <a href="http://people.csail.mit.edu/tjudd/SaliencyBenchmark/">has been shown</a> to improve saliency results in the case of natural images, and it seems that anime images follow this trend too.</p>
<div class="bigcenterimgcontainer">
<div class="caption">
Example of an image on which Hou et al.’s method give bad results (left). Adding a center bias to the saliency map corrects the problem (right).
</div>
</div>
<p>So far, we’ve only dealt with grayscale images. In the case of color images, the original paper suggests applying the algorithm to each color channel separately - typically RGB. It seemed to me that the RGB color space is quite ill-suited to the task, being intended for computer display. Switching to a perceptual color such as <span class="math">\(L^*a^*b^* \)</span> makes a lot more sense for instance, but we can go a bit further than that. Applying <a href="http://en.wikipedia.org/wiki/Principal_component_analysis">principal component analysis</a> on the set of <span class="math">\(L^*a^*b^*\)</span> pixel values for each image individually, we get a color space in which the largest possible variance in perceptual differences is accounted for in each image. It then makes sense to combine the results from each layer by a mean weighted by the corresponding eigenvalue of the covariance matrix of the set of pixel values in the image. Indeed, the largest eigenvalue corresponds to the principal eigenvector, encoding the most information about the pixels of the image.</p>
<p>After this, we simply feed the saliency map to GrabCut, which takes care of removing the extraneous stuff. Well almost. GrabCut expects a mask in which each pixel is associated a value which is either background, probable background, probable foreground or foreground. A simple way to get this information is to threshold the saliency map into these 4 tiers, where lower saliency means closer to background and higher saliency means closer to foreground. Finding the right thresholds is not at all obvious however, so I ran a search on these parameters for my data set, optimizing for highest average area under the ROC curve. If background corresponds to saliency in the <span class="math">\([0;x_1[\)</span> range, probable background within <span class="math">\([x_1;x_2[\)</span>, probable foreground inside <span class="math">\([x_2;x_3[\)</span> and foreground for the remaining <span class="math">\([x_3;1]\)</span>, we found that <span class="math">\(x_1 = \)</span>, <span class="math">\(x_2 = \)</span>, <span class="math">\(x_3 = \)</span> gave the best results. This may be over-fitted to the data, but we really only need the rough area where the character is located anyway so this may work fine.</p>
<h2 id="benchmarks">Benchmarks</h2>
<p>The quality of my background removal method was measured against ground truth information. Yes, I painstakingly removed the background of each image by hand for this. That’s the less fun part of research in the machine learning field when you don’t have graduate students to delegate to :p . Each image is evaluated by its area under the ROC curve against ground truth, and each method is evaluated by its average area over all images.</p>
<p>I compared my method using this metric against: - Raw spectral residual saliency with GrabCut, without center prior. - Center with GrabCut, where the saliency map is just a centered 2D Gaussian. - Entire image with GrabCut, where every pixel is labeled as probably foreground.</p>
<p>(TODO: actually run the numbers and get the results -_-)</p>
<h2 id="future-improvements">Future improvements</h2>
<p>One issue with the saliency map given by my method is the variety in scale of the character - sometimes the character is small in the middle of the image, big with its body taking the entire image, and so huge only part of it fits the image. When the character is too small, the method takes too much background, which is not such a big deal thanks to GrabCut. When it’s too big however, holes are produced in the character.</p>

<div id="disqus_thread"></div>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
<script src="http://code.jquery.com/jquery-1.10.1.min.js"></script>

<script src="../../js/inlineDisqussions.js"></script>
<script src="../../js/disqus.js"></script>

                            </div>
                        </div>
                        <div class="col-md-4" id="toc-wrapper">
                        </div>
                    </div>
                </div>
            </div>
        

            <div id="footer">
                <div class="container">
                    Based on <a href="http://oinkina.github.io/">Lambda Oinks</a> by <a href="https://github.com/oinkina">Oinkina</a>, using 
                    <a href="http://jaspervdj.be/hakyll">Hakyll</a>,
                    <a href="http://getbootstrap.com/">Bootstrap</a>, 
                    <a href="http://www.mathjax.org/">MathJax</a>, and
                    <a href="http://disqus.com/">Disqus</a>.
                </div>
            </div>
        </div>

    <!-- jQuery-->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
    <script src="http://code.jquery.com/jquery-1.10.1.min.js"></script>

    <script src="../../bootstrap/js/bootstrap.min.js"></script>

    <script src="../../highlight/highlight.pack.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>

    <script src="../../js/footnotes.js"></script>
    <script src="../../js/bootstrap-carousel.js"></script>
    <script src="../../js/inlineDisqussions.js"></script>
    <script src="../../js/toc.js"></script>

    <script src="../../js/analytics.js"></script>

    <noscript>Enable JavaScript for footnotes, Disqus comments, and other cool stuff.</noscript>

    </body>

</html>
